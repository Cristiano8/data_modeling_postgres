# Data Engineer Nanodegree - Project 1: Data model with PostgreSQL

- [Data Engineer Nanodegree - Project 1: Data model with PostgreSQL](#data-engineer-nanodegree---project-1-data-model-with-postgresql)
- [Summary](#summary)
- [Dataset](#dataset)
- [Schema](#schema)
- [Files explanation](#files-explanation)
- [1. data](#1-data)
- [2. create_tables.py](#2-create_tables.py)
- [3. etl.ipynb](#3etl.ipynb)
- [4. etl.py](#4etl.py)
- [5. sql_queries.py](#5sql_queries.py)
- [6. test.ipynb](#&test.ipynb)
- [How to run](#how-to-run)

## Summary
A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

## Dataset
The project is based on two dataset:
- **Song dataset**: it is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

   ```
    song_data/A/B/C/TRABCEI128F424C983.json
    song_data/A/A/B/TRAABJL12903CDCF1A.json
   ```
    And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

    ```json
    {
        "num_songs": 1,
        "artist_id": "ARJIE2Y1187B994AB7",
        "artist_latitude": null,
        "artist_longitude": null,
        "artist_location": "",
        "artist_name": "Line Renaud",
        "song_id": "SOUPIRU12A6D4FA1E1",
        "title": "Der Kleine Dompfaff",
        "duration": 152.92036,
        "year": 0
    }
    ```

- **Log Dataset**: it consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

    The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

    ```
    log_data/2018/11/2018-11-12-events.json
    log_data/2018/11/2018-11-13-events.json
    ```

## Schema
In order to analyze and query the data, Sparkify wants to create an OLAP database with PostgreSQL. After the etl process, the star schema resulting in the database will be the following:

**Songoplays table** (Fact table)
| Column name | Type      | Properties        |
| ----------- | --------- | ----------------- |
| songplay_id | SERIAL    | PK, AUTOINCREMENT |
| start_time  | bigint    | NOT NULL          |
| user_id     | varchar   |                   |
| level       | varchar   |                   |
| song_id     | varchar   |                   |
| artist_id   | varchar   |                   |
| session_id  | int       | NOT NULL          |
| location    | varchar   |                   |
| user_agent  | varchar   |                   |

**Users**
| Column name | Type      | Properties |
| ----------- | --------- | ---------- |
| user_id     | varchar   | PK         |
| firsname    | varchar   |            |
| lastname    | varchar   |            |
| gender      | varchar(1)|            |
| level       | varchar   |            |

**Songs**
| Column name | Type    | Properties |
| ----------- | ------- | ---------- |
| song_id     | varchar | PK         |
| title       | varchar | NOT NULL   |
| artist_id   | varchar |            |
| year        | int     |            |
| duration    | float   |            |

**Artists**
| Column name | Type    | Properties |
| ----------- | ------- | ---------- |
| artist_id   | varchar | PK         |
| name        | varchar | NOT NULL   |
| location    | varchar |            |
| latitude    | decimal |            |
| longitude   | decimal |            |


**Time**
| Column name | Type      | Properties |
| ----------- | --------- | ---------- |
| start_time  | bigint    | PK         |
| hour        | int       |            |
| day         | int       |            |
| week        | int       |            |
| month       | int       |            |
| year        | int       |            |
| weekday     | int       |            |


## Files explanation
### 1. data
Directory containing the log files of the project. To have more information about these files look at [Dataset](#dataset)
### 2. create_tables.py
Python file used to create the sparkifydb database, drop the tables if existents and the create the table used in the project.
### 3. etl.ipynb
Jupiter notebook used to execute the ingestion of one file each for song_data and log_data into the tables created using [2. create_tables.py](#2-create-tables.py).
### 4. etl.py
Python file used to populate the tables of the database.
### 5. sql_queries.py
Pyton file containing the queries used to create and populate the tables in the sparkifydb database. The file is divided into DROP TABLES queries, CREATE TABLES queries, INSERT INTO queries, the FIND SONG query to retrieve song_id and artist_id based on logs json files data and the definition of the QUERY LISTS that are imported and executed by [2. create_tables.py](#2-create-tables.py). The queries are simple strings executed using the cursor class created from the connection class of psycopg2 library.
### 6. test.ipynb
Jupiter notebook to test if the tables have been correctly populated.


## How to run
cd <path of the project>
python create_tables.py
python etl.py





